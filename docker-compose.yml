services:
  # ------------------- ZOOKEEPER -------------------
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
  # zookeeper:
  #   image: wurstmeister/zookeeper
  #   container_name: zookeeper
  #   ports:
  #     - "2181:2181"

  # ------------------- KAFKA -------------------
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_LOG_RETENTION_HOURS: 24

  # kafka:
  #   image: wurstmeister/kafka
  #   container_name: kafka
  #   ports:
  #     - "9092:9092"
  #   environment:
  #     KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
  #     KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
  #     KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
  #     KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
  #     KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
  #     KAFKA_CREATE_TOPICS: "opensky:1:1,airdata_raw:1:1"
  #     KAFKA_LOG_RETENTION_BYTES: 1073741824
  #   depends_on:
  #     - zookeeper
  #   volumes:
  #     - /var/run/docker.sock:/var/run/docker.sock

  # ------------------- HDFS NAMENODE -------------------
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - 9870:9870
      - 9000:9000
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
      - ./myhadoop:/myhadoop
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop.env

  # ------------------- HDFS DATANODE -------------------
  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env

# # ---------- Create topic helper (one-shot) ----------
#   create-topic:
#     image: wurstmeister/kafka:latest
#     container_name: create-topic
#     entrypoint: ["/bin/sh","-c"]
#     # waits for kafka to be ready, then create topic
#     command: |
#       "sleep 6 &&
#        kafka-topics.sh --create --topic airdata_raw --bootstrap-server kafka:9092 --replication-factor 1 --partitions 1 || true &&
#        echo 'topic created/exists' &&
#        sleep 2"
#     depends_on:
#       - kafka

  # ---------- PRODUCER (fetch OpenSky → Kafka) ----------
  producer:
    build:
      context: ./services/producer
      dockerfile: Dockerfile
    container_name: opensky-producer
    restart: unless-stopped # seconds between API pulls
    depends_on:
      - kafka


  # ---------- CONSUMER (Kafka → HDFS raw + processed) ----------
  consumer:
    build:
      context: ./services/consumer
      dockerfile: Dockerfile
    container_name: opensky-consumer
    restart: unless-stopped
    depends_on:
      - kafka
      - namenode

  # ---------- DASHBOARD (Streamlit) ----------
  dashboard:
    build:
      context: ./services/dashboard
      dockerfile: Dockerfile
    container_name: opensky-dashboard
    restart: unless-stopped
    ports:
      - "8501:8501"
    environment:
      - HDFS_URL=http://namenode:9870
      - HDFS_USER=root
      - PROCESSED_DIR=/opensky/processed
    depends_on:
      - namenode
      - consumer

# ------------- VOLUMES -----------------
volumes:
  hadoop_datanode:
  hadoop_namenode:

networks:
  default:
    name: sky-net
    external: true