ğŸŒ OpenSky Real-Time DataLake Pipeline

A complete end-to-end Big Data architecture using Kafka, Hadoop HDFS, Python, Docker, and Streamlit.

----------------------------------------------------
ğŸ“˜ Table of Contents
----------------------------------------------------

Overview

Architecture

Data Flow

Tech Stack

Project Structure

How to Run

Service Details

Dashboard

Testing

Troubleshooting

Contributors

----------------------------------------------------
ğŸŒ Overview
----------------------------------------------------

This project implements a production-style DataLake architecture for real-time aircraft tracking data using the OpenSky API.

It follows enterprise Big Data concepts:

âœ” Ingestion Layer: Python Producer â†’ Kafka
âœ” Persistence Layer: HDFS (raw + processed zones)
âœ” Processing Layer: Kafka Consumer performing ETL
âœ” Analytics Layer: Streamlit dashboard reading HDFS

The system is fully containerized using Docker Compose.

----------------------------------------------------
ğŸ› Architecture
----------------------------------------------------
 ![alt text](<architecture 3 layer.svg>)
----------------------------------------------------
ğŸ” Data Flow
----------------------------------------------------
1ï¸âƒ£ Producer â†’ Kafka

Fetches aircraft data from OpenSky REST API

Sends raw JSON to Kafka topic opensky

2ï¸âƒ£ Kafka â†’ Consumer â†’ HDFS

Consumer listens to Kafka topic

Saves raw data in /opensky/raw

Cleans & transforms data (ETL)

Saves processed data in /opensky/processed

3ï¸âƒ£ Streamlit Dashboard

Reads processed data from HDFS

Displays metrics, graphs, maps, and filters

----------------------------------------------------
ğŸ›  Tech Stack
----------------------------------------------------
Layer	Technology	Purpose
Ingestion	Python, Requests	Fetch OpenSky API
Messaging	Kafka + Zookeeper	Streaming pipeline
Storage	HDFS (Hadoop 3.2.1)	DataLake zones
Processing	Python Consumer	ETL (raw â†’ processed)
Analytics	Streamlit	Interactive dashboard
Orchestration	Docker Compose	Service orchestration
----------------------------------------------------
ğŸ“ Project Structure
----------------------------------------------------
FINAL_PROJECT/
â”‚
â”œâ”€â”€ docker-compose.yml              # Orchestration
â”œâ”€â”€ hadoop.env                      # Hadoop config
â”œâ”€â”€ requirements.txt                # Python libs
â”‚
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ producer/
â”‚   â”‚   â”œâ”€â”€ producer.py
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”œâ”€â”€ consumer/
â”‚   â”‚   â”œâ”€â”€ consumer.py
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â””â”€â”€ dashboard/
â”‚       â”œâ”€â”€ dashboard.py
â”‚       â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ myhadoop/                       # HDFS local mount
â”‚
â”œâ”€â”€ test_hdfs.py
â”œâ”€â”€ test_kafka.py
â””â”€â”€ test_imports.py

----------------------------------------------------
â–¶ï¸ How to Run the Project
----------------------------------------------------
0. Install prerequisites

Docker

Docker Compose

Python 3.10+ (optional for local tests)

1. Create Docker network
docker network create sky-net

2. Start the complete system
docker-compose up --build -d

3. Check running containers
docker ps


Expected services:

Service	Status
namenode	Running
datanode	Running
kafka	Running
zookeeper	Running
opensky-producer	Running
opensky-consumer	Running
opensky-dashboard	Running
4. Access important UIs
Component	URL
Dashboard	http://localhost:8501

HDFS Namenode UI	http://localhost:9870

Kafka	9092
Zookeeper	2181
----------------------------------------------------
ğŸ“¦ Service Details
----------------------------------------------------
ğŸ›« Producer (OpenSky â†’ Kafka)

Path: services/producer/producer.py

Calls OpenSky API every X seconds

Parses JSON payload

Sends message to Kafka topic opensky

ğŸ”„ Consumer (Kafka â†’ HDFS)

Path: services/consumer/consumer.py

Reads Kafka messages

Writes raw JSON â†’ /opensky/raw

Cleaned + transformed data â†’ /opensky/processed

Uses WebHDFS API

ğŸ§± HDFS (Hadoop DataLake)

Directories:

/opensky/raw
/opensky/processed


Namenode UI: http://localhost:9870

ğŸ“Š Streamlit Dashboard

Path: services/dashboard/dashboard.py

Features:

âœ” Explore raw or processed data
âœ” Aircraft statistics
âœ” Search + filtering
âœ” Real-time update button
âœ” Data preview tables
âœ” Graphs + charts

----------------------------------------------------
ğŸ“ˆ Dashboard
----------------------------------------------------

Open the dashboard:

ğŸ‘‰ http://localhost:8501

Shows:

Number of active flights

Altitude distribution

Country-based filtering

Map (optional)

Raw vs processed data quality

Custom analytics